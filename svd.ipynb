{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.sparse import lil_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vertices = pd.read_csv('vertices.csv')\n",
    "edges = pd.read_csv('edges.csv')\n",
    "ids = pd.read_csv('ids.csv')\n",
    "new_vertices = pd.read_csv('new_vertices.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building sparse matrix\n",
    "def build_matr():\n",
    "    A = lil_matrix((len(vertices), len(vertices)))\n",
    "    for row in edges.values:\n",
    "        A[int(row[0])-1, int(row[1])-1] = 1 \n",
    "        A[int(row[1])-1, int(row[0])-1] = 1 \n",
    "    return(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = build_matr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "svd = TruncatedSVD(n_components=450,algorithm='randomized', random_state=42)\n",
    "transformed = svd.fit_transform(A)\n",
    "components = svd.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('components.npy', components)\n",
    "np.save('transformed.npy', transformed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weights assigning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summa = new_vertices[new_vertices.id.isin(ids.id)].count_connections.sum()\n",
    "df = ids.merge(new_vertices[['id', 'count_connections']], on='id')\n",
    "df['k'] = df.count_connections/summa\n",
    "df['k']*=100000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### hop 2-3  and search functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessed data here https://drive.google.com/file/d/1a0Gbzp1U251twbcr6MUuqrOBIY4bB-Bz/view\n",
    "all_pairs = pd.read_csv('all_connections')\n",
    "index = all_pairs.id_1\n",
    "connections = all_pairs.connection\n",
    "inc = dict(zip(index, map(eval, connections)))\n",
    "\n",
    "def BFS1(root):\n",
    "    visited = set()\n",
    "    visited.add(root)\n",
    "    res = set()\n",
    "    for neighbour in inc[root]:\n",
    "         if neighbour not in visited: \n",
    "            res.add(neighbour)\n",
    "    return(visited, res)\n",
    "def BFS2(visited, q):\n",
    "    res = set()\n",
    "    for i in q:\n",
    "        visited.add(i)\n",
    "        for neighbour in inc[i]:\n",
    "             if neighbour not in visited: \n",
    "                    res.add(neighbour)\n",
    "    return(visited, res)\n",
    "def BFS3(visited, q):\n",
    "    res = set()\n",
    "    for i in q:\n",
    "        visited.add(i)\n",
    "        for neighbour in inc[i]:\n",
    "             if neighbour not in visited: \n",
    "                    res.add(neighbour)\n",
    "    return(visited, res)\n",
    "def hop3(root):\n",
    "    v, q = BFS1(root)\n",
    "    v,q = BFS2(v,q)\n",
    "    v,q = BFS3(v,q)\n",
    "    return(q)\n",
    "def hop2(root):\n",
    "    v, q = BFS1(root)\n",
    "    v,q = BFS2(v,q)\n",
    "    return(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itog = pd.DataFrame({'id_1':[], 'id_2':[], 'prob':[]})\n",
    "for i in tqdm.tqdm(ids.id):\n",
    "    k = df[df.id==i].k.sum()*100000\n",
    "    q = i-1\n",
    "    check = new_vertices[(new_vertices['id'].isin(hop3(i)) | new_vertices['id'].isin(hop2(i)))].id\n",
    "    probs = transformed[q]@components[:,check-1]\n",
    "    probs= abs(probs-1)\n",
    "    res = pd.DataFrame({'id_2':check, 'prob':probs})\n",
    "    res['id_1'] = i\n",
    "    res['min'], res['max'] = res[['id_1', 'id_2']].min(axis=1),res[['id_1', 'id_2']].max(axis=1) \n",
    "    res = res.drop(columns=['id_1', 'id_2']).rename(columns={'min':'id_1', 'max':'id_2'}).sort_values('prob')\n",
    "    itog = itog.append(res[['id_1', 'id_2', 'prob']][:int(1.5*k)+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def right_form(matr):\n",
    "    return(edges[['id_1', 'id_2']].append(matr[['id_1', 'id_2']], sort=False).drop_duplicates()[len(edges):])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "right_form(itog).drop_duplicates().astype(int)[:100000].to_csv('svdResult.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
